{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alicmu2024/GNSS-Jamming-Detection-and-Classification-using-Machine-Learning-Deep-Learning-and-Computer-Vision/blob/main/Jamming_ML_Models_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`Let's move on to the Machine Learning `**"
      ],
      "metadata": {
        "id": "FIZnysRrGOZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading The Dataset\n",
        "%%capture\n",
        "! wget https://zenodo.org/records/3783969/files/Jamming_Classifier.zip?download=1"
      ],
      "metadata": {
        "id": "srcC3hjz5sHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%cp /content/Jamming_Classifier.zip?download=1 Jamming_Classifier.zip\n",
        "! rm -rf /content/Jamming_Classifier.zip?download=1\n",
        "! unzip ./Jamming_Classifier.zip\n",
        "! rm -rf /content/energyop.m\n",
        "! rm -rf /content/Channel\n",
        "! rm -rf /content/Classifier\n",
        "! rm -rf /content/Detector_Functions\n",
        "! rm -rf /content/GNSS_signals\n",
        "! rm -rf /content/Init\n",
        "! rm -rf /content/Jammer_signals\n",
        "! rm -rf /content/Misc\n",
        "! rm -rf /content/Plotting\n",
        "! rm -rf /content/Progress_Bar\n",
        "! rm -rf /content/Sim\n",
        "! rm -rf /content/Test_statistic\n",
        "! rm -rf /content/CNN_algorithm.m\n",
        "! rm -rf /content/Jamming_Classifier.zip\n",
        "! rm -rf /content/SVM_algorithm.m\n",
        "! rm -rf /content/SVM_algorithm2.m\n",
        "! rm -rf /content/Testing_CNR_JSR.mat\n",
        "! rm -rf /content/Training_CNR_JSR.mat\n",
        "! rm -rf /content/energyop.m\n",
        "! rm -rf /content/main.m\n",
        "! rm -rf /content/main_test_classifiers.m\n",
        "! rm -rf /content/parfor_progress_percentage.txt\n",
        "! rm -rf /content/plotConfMat.m\n",
        "! rm -rf /content/stopIfAccuracyNotImproving.m"
      ],
      "metadata": {
        "id": "MXumBtVa5r48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms as tt\n",
        "from collections import Counter\n",
        "from torch.utils.data import Subset"
      ],
      "metadata": {
        "id": "hMa2HqzYGN3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your dataset directories\n",
        "train_dir = '/content/Image_training_database'\n",
        "test_dir = '/content/Image_testing_database'\n",
        "\n",
        "# Data transforms (normalization & data augmentation)\n",
        "stats = ((0.7582, 0.7582, 0.7582), (0.4282, 0.4282, 0.4282))\n",
        "train_tfms = tt.Compose([\n",
        "    tt.Resize(128),\n",
        "    tt.ToTensor(),\n",
        "    tt.Normalize(*stats, inplace=True)\n",
        "])\n",
        "\n",
        "valid_tfms = tt.Compose([\n",
        "    tt.Resize(128),\n",
        "    tt.ToTensor(),\n",
        "    tt.Normalize(*stats)\n",
        "])\n",
        "\n",
        "# Load the datasets\n",
        "train_ds = ImageFolder(train_dir, train_tfms)\n",
        "test_ds = ImageFolder(test_dir, valid_tfms)\n",
        "\n",
        "# Combine the datasets\n",
        "combined_ds = train_ds + test_ds\n",
        "\n",
        "# Total number of images in the combined dataset\n",
        "total_images = len(combined_ds)\n",
        "print('Total number of images: ', total_images)\n",
        "\n",
        "# Get the class labels\n",
        "class_labels = [combined_ds[i][1] for i in range(total_images)]\n",
        "class_counts = Counter(class_labels)\n",
        "print(\"Class distribution in combined dataset:\", class_counts)\n",
        "\n",
        "# Stratified split\n",
        "train_indices, temp_indices = train_test_split(\n",
        "    list(range(total_images)),\n",
        "    test_size=30000,\n",
        "    stratify=class_labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "val_indices, test_indices = train_test_split(\n",
        "    temp_indices,\n",
        "    test_size=15000,\n",
        "    stratify=[class_labels[i] for i in temp_indices],\n",
        "    random_state=42)\n",
        "\n",
        "# Create subsets for training, validation, and test datasets\n",
        "train_ds = Subset(combined_ds, train_indices)\n",
        "valid_ds = Subset(combined_ds, val_indices)\n",
        "test_ds = Subset(combined_ds, test_indices)\n",
        "\n",
        "# Check class balance in the training dataset\n",
        "train_classes = [combined_ds[i][1] for i in train_indices]\n",
        "train_class_counts = Counter(train_classes)\n",
        "print(\"Training dataset class balance:\")\n",
        "print(train_class_counts)\n",
        "\n",
        "# Check class balance in the validation dataset\n",
        "valid_classes = [combined_ds[i][1] for i in val_indices]\n",
        "valid_class_counts = Counter(valid_classes)\n",
        "print(\"\\nValidation dataset class balance:\")\n",
        "print(valid_class_counts)\n",
        "\n",
        "# Check class balance in the test dataset\n",
        "test_classes = [combined_ds[i][1] for i in test_indices]\n",
        "test_class_counts = Counter(test_classes)\n",
        "print(\"\\nTest dataset class balance:\")\n",
        "print(test_class_counts)\n",
        "\n",
        "# PyTorch data loaders\n",
        "batch_size = 32\n",
        "train_dl = DataLoader(train_ds,\n",
        "                      batch_size,\n",
        "                      shuffle=True,\n",
        "                      num_workers=2,\n",
        "                      pin_memory=True)\n",
        "\n",
        "valid_dl = DataLoader(valid_ds,\n",
        "                      batch_size,\n",
        "                      num_workers=2,\n",
        "                      pin_memory=True)\n",
        "\n",
        "test_dl = DataLoader(test_ds,\n",
        "                     batch_size,\n",
        "                     num_workers=2,\n",
        "                     pin_memory=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blLpIc2B6TLp",
        "outputId": "411b0993-93c7-4901-adfc-ed01ca325941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of images:  120000\n",
            "Class distribution in combined dataset: Counter({0: 20000, 1: 20000, 2: 20000, 3: 20000, 4: 20000, 5: 20000})\n",
            "Training dataset class balance:\n",
            "Counter({0: 15000, 3: 15000, 5: 15000, 1: 15000, 4: 15000, 2: 15000})\n",
            "\n",
            "Validation dataset class balance:\n",
            "Counter({5: 2500, 1: 2500, 0: 2500, 4: 2500, 2: 2500, 3: 2500})\n",
            "\n",
            "Test dataset class balance:\n",
            "Counter({2: 2500, 4: 2500, 0: 2500, 3: 2500, 1: 2500, 5: 2500})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.models import ResNet18_Weights\n",
        "\n",
        "# Define the FeatureExtractor class\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "        self.model = nn.Sequential(*list(self.model.children())[:-1])  # Remove the last layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            features = self.model(x)\n",
        "        return features.view(features.size(0), -1)\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Extract features function\n",
        "def extract_features(data_loader):\n",
        "    feature_extractor = FeatureExtractor().to(device)  # Move model to device\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for images, target in data_loader:\n",
        "        images = images.to(device)  # Move images to the same device as the model\n",
        "        feature = feature_extractor(images)\n",
        "        features.append(feature.cpu().numpy())  # Move features back to CPU\n",
        "        labels.append(target.numpy())  # Assuming target is already a numpy array\n",
        "\n",
        "    return np.concatenate(features), np.concatenate(labels)"
      ],
      "metadata": {
        "id": "NjBt1L0hHF-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = extract_features(train_dl)\n",
        "print(features.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3vUroI-MicT",
        "outputId": "a7212749-369b-4d48-9b65-b93546d058cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 105MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90000, 512)\n",
            "(90000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features,\n",
        "                                                    labels,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)"
      ],
      "metadata": {
        "id": "fQ0nG3cKBf5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=10000)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the models\n",
        "for model in [rf_model]:\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNwcxG6qISii",
        "outputId": "5486266e-35c1-460c-b400-f6701a95cbdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99      2059\n",
            "           1       0.98      0.99      0.98      1997\n",
            "           2       1.00      1.00      1.00      1983\n",
            "           3       0.81      0.94      0.87      1999\n",
            "           4       0.99      0.97      0.98      1939\n",
            "           5       0.95      0.78      0.86      2023\n",
            "\n",
            "    accuracy                           0.95     12000\n",
            "   macro avg       0.95      0.95      0.95     12000\n",
            "weighted avg       0.95      0.95      0.95     12000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train SVM\n",
        "svm_model = SVC(kernel='linear')  # You can choose other kernels as well\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the models\n",
        "for model in [svm_model]:\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9gaOkotSahQ",
        "outputId": "6d44c069-50c3-4f76-86b4-beafd0449054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99      2059\n",
            "           1       0.98      0.99      0.99      1997\n",
            "           2       1.00      1.00      1.00      1983\n",
            "           3       0.80      0.94      0.86      1999\n",
            "           4       1.00      0.98      0.99      1939\n",
            "           5       0.95      0.77      0.85      2023\n",
            "\n",
            "    accuracy                           0.95     12000\n",
            "   macro avg       0.95      0.95      0.95     12000\n",
            "weighted avg       0.95      0.95      0.95     12000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train XGBoost\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the models\n",
        "for model in [xgb_model]:\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZblADHbTKoZ",
        "outputId": "b522a128-a744-4798-f68c-3bb14b838340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      2059\n",
            "           1       1.00      1.00      1.00      1997\n",
            "           2       1.00      1.00      1.00      1983\n",
            "           3       0.85      0.90      0.87      1999\n",
            "           4       1.00      0.99      1.00      1939\n",
            "           5       0.90      0.84      0.87      2023\n",
            "\n",
            "    accuracy                           0.95     12000\n",
            "   macro avg       0.96      0.96      0.96     12000\n",
            "weighted avg       0.96      0.95      0.95     12000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
